---
layout: post
title: RAID 1 to RAID 5
date: '2006-10-04T10:54:00.000+02:00'
author: Steven Van Acker
tags: 
modified_time: '2008-01-17T09:21:49.251+01:00'
blogger_id: tag:blogger.com,1999:blog-5513234137363262204.post-8663063463843068077
blogger_orig_url: http://www.singularity.be/2006/10/raid-1-to-raid-5.html
---

Since the schoolyear started, I have been fighting against a tidal wave of incoming logmessages. People obviously started using the network again.<br /><br />I decided to replace the current 72GB RAID1 set with a 144GB RAID5 set (using 3 72GB harddisks).<br /><br /><a href='http://www.tldp.org/HOWTO/Software-RAID-HOWTO-6.html'>http://www.tldp.org/HOWTO/Software-RAID-HOWTO-6.html</a><br /><br />The first thing I noticed was that the kernel was not compiled with RAID5 support, just RAID1. This server is one of those who is still running some hybrid version of an old Debian release overwritten with countless compilations from source, and no documentation about it at all.<br />In addition, the kernel it was running was 2.4.26 with a bunch of unspecified patches.<br /><br />I installed a brand new 2.6.17.13 kernel on it, with all RAID-levels supported (standard kernel for all new machines), but the server wouldn't load the networkmodules. I'm guessing this is because modprobe is too old...<br /><br />So, resorting to a desperate option, I recompiled the kernel from local source, just adding RAID5.<br /><br />After a few disturbing incidents (the machine crashed when I wiped one of the RAID1 disks, after carefully removing it from the RAID set), I'm back again and about to continue...<br /><br />The situation is as follows: /dev/sda and /dev/sdc contains the partitions of /, /boot and swap. All in RAID1. /dev/sdb and /dev/sdd both contain 1 partition (sdb1 and sdd1) and form a RAID1 set together. /dev/sde is the new disk.<br /><br />I started by removing /dev/sdd1 from the RAID set by marking it as faulty and the removing it:<br /><pre><br />mdadm --manage --set-faulty /dev/md3 /dev/sdd1<br />mdadm /dev/md3 -r /dev/sdd1<br /></pre><br /><br />I then started wiping /dev/sdd, but for some reason this crashed the machine. So, moving along...<br />the idea is to have /dev/sdb, /dev/sdd and /dev/sde in the RAID5 set. But I want to have /dev/sdb running as long as possible. So I will create a degraded RAID5 with /dev/sdd and /dev/sde first, copy the data and then add /dev/sdb later.<br /><br />I partitioned both /dev/sdd and /dev/sde the same as /dev/sdb.<br /><pre><br />sfdisk -d /dev/sdb &gt; /tmp/x<br />cat /tmp/x | sfdisk -f /dev/sdd<br />cat /tmp/x | sfdisk -f /dev/sde<br /></pre><br /><br />mdadm --create --verbose /dev/md4 --level=5 --raid-devices=3 missing /dev/sdd1 /dev/sde1<br />